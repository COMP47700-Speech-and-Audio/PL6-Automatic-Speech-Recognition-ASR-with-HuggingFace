{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXz2nXUpo7XA"
   },
   "source": [
    "# COMP47700 Speech and Audio PL6: Automatic Speech Recognition (ASR) with HuggingFace\n",
    "---\n",
    "\n",
    "## Learning outcomes\n",
    "This practical tutorial covers the following learning outcomes within the COMP47700 Speech and Audio module:\n",
    "* Recognise and reflect on the relationship between the underlying theories and start-of-the-art research (**LO4**)\n",
    "  * Familiarise yourself with current available frameworks and tools for ASR (HuggingFace, Whisper)\n",
    "  * Load and test SotA models for ASR using HuggingFace\n",
    "* Create programmes to conduct experiments on speech and audio samples building on third software libraries (**LO6**)\n",
    "  * Evaluate model's performance for ASR\n",
    "  * Familiarise yourself with current available datasets for ASR tasks\n",
    "\n",
    "\n",
    "## Module topics\n",
    "This practical tutorial builds on the following core topics:\n",
    "* Automatic speech recognition and speaker recognition (Unit 9)\n",
    "\n",
    "## Why is it important?\n",
    "* Understanding ASR and speaker recognition is crucial for developing a wide range of applications, from voice-controlled interfaces to security systems and personalized user experiences. These technologies play a key role in enhancing human-computer interaction and making technology more versatile and accessible.\n",
    "* Getting familiar with SoTA frameworks like HuggingFace is essential for leveraging the platform effectively, developing customized models, integrating with other NLP tools, and contributing to the collaborative and dynamic community focused on advancing speech-related tasks, natural language processing and machine learning.\n",
    "\n",
    "## Structure of this tutorial\n",
    "This practical tutorial contains different sections:\n",
    "* **Live coding:** Basic theory, demos and coding examples presented by the lecturer on site (unmarked)\n",
    "* **Student activity:** Familiarisation and coding exercises to be completed by the students and followed by a short discussion on site (unmarked). These activites introduce key concepts and skills necessary to complete the assignments.\n",
    "* **Assignment:** Three (3) take home problem/coding questions to be completed by the students and due in two (2) weeks from the day the practical tutorial is given. Assignment questions represent fifteen (15) mark points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IK3l5EKEPB5"
   },
   "source": [
    "## Setup notes\n",
    "We will be using Google Colabs for our labs but if you wish to run speech and audio projects locally (not recommended) you will need a manage your own Python environment setup with a number of important packages.\n",
    "\n",
    "Some important libraries and packages for signal analysis in Python are:\n",
    "\n",
    "[datasets](https://huggingface.co/docs/datasets/index): library for accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks.\n",
    "\n",
    "[transformers](https://huggingface.co/docs/transformers/index): provides APIs and tools to download and train state-of-the-art pretrained models.\n",
    "\n",
    "[evaluate](https://huggingface.co/docs/evaluate/index): library for evaluating ML models and datasets.\n",
    "\n",
    "[ffmpeg](https://python-ffmpeg.readthedocs.io/en/latest/): is a powerful and versatile multimedia processing tool that can handle a variety of audio and video formats. Provides a Python wrapper for the FFmpeg library, allowing developers to use FFmpeg functionality in Python scripts.\n",
    "\n",
    "**To install the required libraries, execute the command-line below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNJW8NItJtsK"
   },
   "outputs": [],
   "source": [
    "!pip install datasets transformers huggingface_hub torchaudio librosa jiwer evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSELc702SXYs"
   },
   "source": [
    "---\n",
    "### **Live coding:** Collecting data from zipped file\n",
    "1. From your local system, select the .zip file provided for PL6 (`PL6_files.zip`).\n",
    "2. Use `zipfile` to extract the files to your Google Colab environment.\n",
    "\n",
    "**Notes:** You can inspect the extracted folder (phonemes) in the files section at the table of contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhyy3UKXSklR"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from google.colab import files\n",
    "\n",
    "zipname = 'PL6_files.zip'\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eicYmeSSpXa"
   },
   "outputs": [],
   "source": [
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zipname, 'r') as zip_ref:\n",
    "  zip_ref.extractall()  # Extract all files to the current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpkywgHG1Fxe"
   },
   "source": [
    "---\n",
    "### **Live coding:** Inference Examples with HuggingFace\n",
    "HuggingFace contains over 7000 ASR models available at **The Hub**. They can be used through the inference API or using libraries such as `transformers, speechbrain` and `espnet`. In this tutorial, we will show examples using the inference API and the `transformers` library.\n",
    "\n",
    "1. Define an inference query function\n",
    "2. Load a sample wave file\n",
    "3. Test the **wav2vec 2.0** model from api-inference.huggingface.co\n",
    "4. Test the **wav2vec 2.0** model loading the model from the `transformers` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axinpQE2QBJZ"
   },
   "outputs": [],
   "source": [
    "# query function requires and API_TOKEN from HuggingFace\n",
    "def query(filename, headers):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    return json.loads(response.content.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcsL2Pkx1L69"
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import urllib.request as urllib2\n",
    "import IPython\n",
    "import ipywidgets as widgets\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Load sample file\n",
    "h_comp='hinesCOMP47700.wav'\n",
    "\n",
    "# Set API_TOKEN from HuggingFace [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]\n",
    "headers = {\"Authorization\": f\"Bearer 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\"}\n",
    "\n",
    "# Call the 'facebook/wav2vec2-base-960h' pre-trained model from the api-inference.huggingface portal\n",
    "API_URL = \"https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h\"\n",
    "\n",
    "data = query(\"hinesCOMP47700.wav\", headers)\n",
    "print(data['text'])\n",
    "ipd.Audio(filename=h_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXGDcgnpFeFq"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\")\n",
    "data = pipe(\"hinesCOMP47700.wav\")\n",
    "print(data['text'])\n",
    "ipd.Audio(filename=h_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fkub41TLS3eY"
   },
   "source": [
    "### **Student activity #1:** Inference for multiple files\n",
    "1. Find the folder `tcdvoip_sample` containing a small set of wavefiles and create a dataframe named `dftranscriptions` listing the files names under the column `file`\n",
    "2. Use the **wav2vec 2.0** model to generate the transcriptions for the list of files and store them in the dataframe under the column `stt_wav2vec2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZswSKnqi4xbn"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "## Student activity solution #1\n",
    "###############################\n",
    "\n",
    "import requests, zipfile, io, os\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set a dataframe for our dataset\n",
    "basedir='./tcdvoip_sample/'\n",
    "wavefilenames = listdir(basedir)\n",
    "wavefilenames = sorted(wavefilenames)\n",
    "dftranscriptions = pd.DataFrame(columns = ['file'])\n",
    "dftranscriptions['file'] = wavefilenames\n",
    "dftranscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x03kjptwUIEG"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "## Student activity solution #1\n",
    "###############################\n",
    "\n",
    "# set task and model parameters for pipeline function\n",
    "task = \"automatic-speech-recognition\"\n",
    "modelname = 'facebook/wav2vec2-base-960h'\n",
    "pipe = pipeline(task, modelname)\n",
    "\n",
    "stranscription = []\n",
    "\n",
    "# process wavefiles\n",
    "for fname in wavefilenames:\n",
    "  transcription = pipe(basedir+fname)\n",
    "  stranscription.append(transcription['text'])\n",
    "\n",
    "dftranscriptions['stt_wav2vec2'] = stranscription\n",
    "dftranscriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Dw4qEUXXFkJ"
   },
   "source": [
    "### The Hub\n",
    "[The Hub](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads) is a cloud-based platform provided by HuggingFace that serves as a central repository for sharing and versioning models and datasets. It allows to upload, download, and share pre-trained models for NLP, Computer vision, and Audio/Speech tasks.\n",
    "\n",
    "\n",
    "For ASR, over 7000 models are available at [The Hub](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads). They vary in the type of libraries (e.g., transformers, PyTorch, ESPnet, TensorFlow, SpeechBrain, etc), languages (e.g., English, German, Spanish, Hindi, Chinese, etc.). In this tutorial, we will test four different pre-trained models from the HuggingFace repository.\n",
    "\n",
    "* **'facebook/wav2vec2-base-960h'**: model description [here](https://huggingface.co/facebook/wav2vec2-base-960h)\n",
    "* **'facebook/hubert-large-ls960-ft'**: model description [here](https://huggingface.co/facebook/hubert-large-ls960-ft)\n",
    "* **'openai/whisper-base'**: model description [here](https://huggingface.co/openai/whisper-base)\n",
    "* **'openai/whisper-tiny'**: model description [here](https://huggingface.co/openai/whisper-tiny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FrQYRXzWAUX"
   },
   "source": [
    "### **Live coding:** Exploring ASR Models in HuggingFace\n",
    "\n",
    "1. Define a function to compute the transcriptions for a list of wavefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tYTAxhxdb4w"
   },
   "outputs": [],
   "source": [
    "# test a huggingface model from a list of wavefiles, a filepath, and the model's path in The Hub\n",
    "# returns a list with the transcriptions\n",
    "\n",
    "def testHugModelwavlist(wavelist, filepath, hfmodelname):\n",
    "  # set pipe with target task\n",
    "  task = \"automatic-speech-recognition\"\n",
    "  hfmodel = hfmodelname\n",
    "  pipe = pipeline(task, hfmodel)\n",
    "\n",
    "  stranscription = []\n",
    "\n",
    "  # Process wavfiles\n",
    "  for fname in wavelist:\n",
    "    transcription = pipe(filepath+fname)\n",
    "    stranscription.append(transcription['text'])\n",
    "\n",
    "  return stranscription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGfA0i4U-llk"
   },
   "source": [
    "### **Student activity #2:** Inference for multiple models\n",
    "1. Use the function `testHugModelwavlist` to generate transcriptions for the files listed in the dataframe `dftranscriptions`. Load these models hosted in Huggingface's hub to generate those transcriptions:\n",
    "- facebook/wav2vec2-base-100h\n",
    "- facebook/hubert-large-ls960-ft\n",
    "- openai/whisper-base\n",
    "- openai/whisper-tiny\n",
    "2. Store the generated transcriptions in the dataframe `dftranscriptions` under the columns:\n",
    "- facebook/wav2vec2-base-100h `sst_wav2vec2-base`\n",
    "- facebook/hubert-large-ls960-ft `sst_hubert`\n",
    "- openai/whisper-base `sst_whisper-base`\n",
    "- openai/whisper-tiny `sst_whisper-tiny`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "um-rqsaM9vjv"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "## Student activity solution #2\n",
    "###############################\n",
    "\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set a dataframe for our dataset\n",
    "basedir='./tcdvoip_sample/'\n",
    "wavefilenames = listdir(basedir)\n",
    "wavefilenames = sorted(wavefilenames)\n",
    "#dftranscriptions = pd.DataFrame(columns = ['file'])\n",
    "#dftranscriptions['file'] = wavefilenames[:6]\n",
    "\n",
    "# call the test function and add the results to our dataframe\n",
    "stt = testHugModelwavlist(wavefilenames[:6], basedir, 'facebook/wav2vec2-base-100h')\n",
    "dftranscriptions['sst_wav2vec2-base'] = stt\n",
    "\n",
    "stt = testHugModelwavlist(wavefilenames[:6], basedir, 'facebook/hubert-large-ls960-ft')\n",
    "dftranscriptions['stt_hubert'] = stt\n",
    "\n",
    "stt = testHugModelwavlist(wavefilenames[:6], basedir, 'openai/whisper-base')\n",
    "dftranscriptions['stt_whisper-base'] = stt\n",
    "\n",
    "stt = testHugModelwavlist(wavefilenames[:6], basedir, 'openai/whisper-tiny')\n",
    "dftranscriptions['stt_whisper-tiny'] = stt\n",
    "\n",
    "dftranscriptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCTmyjixloHH"
   },
   "source": [
    "### Word Error Rate\n",
    "For this tutorial, we will evaluate the performance of the loaded models with the **Word Error Rate (WER)**. WER is a common metric of the performance of an ASR system.\n",
    "\n",
    "WER can be computed as:\n",
    "* WER = (S + D +I) / N\n",
    "* WER = (S + D +I) / (S + D + C)\n",
    "\n",
    "where\n",
    "\n",
    "* **S**: number of substitutions\n",
    "* **D**: number of deletions\n",
    "* **I**: number of insertions\n",
    "* **C**: number of correct words\n",
    "* **N**: number of words in the reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B45Ies4JDyME"
   },
   "source": [
    "### **Live coding:** Evaluate Model's performance\n",
    "\n",
    "1. Add the transcription references to our wavefiles dataframe\n",
    "2. Define a function to compute the WER for our model's transcriptions\n",
    "3. Observe the resulting performance and handle issues (uppercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nBqhc5r8ieP"
   },
   "outputs": [],
   "source": [
    "# Add column with reference text\n",
    "reference = ['HIS HIP STRUCK THE KNEE OF THE NEXT PLAYER THERE IS A LAG BETWEEN THOUGHT AND ACT',\n",
    "             'GREEN ICE FROSTED THE PUNCH BOWL THE STEADY DRIP IS WORSE THAN A DRENCHING RAIN',\n",
    "             'WHEN THE FROST HAS COME IT IS TIME FOR TURKEY LOOP THE BRAID TO THE LEFT AND THEN OVER',\n",
    "             'HOIST THE LOAD TO YOUR LEFT SHOULDER BOTH LOST THEIR LIVES IN THE RAGING STORM',\n",
    "             'POST NO BILLS ON THIS OFFICE WALL THEY SANG THE SAME TUNES AT EACH PARTY',\n",
    "             'A CRUISE IN WARM WATERS IN A SLEEP YACHT IS FUN TEAR A THIN SHEET FROM THE YELLOW PAD'\n",
    "             ]\n",
    "\n",
    "dftranscriptions['reference'] = reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heLElpnUijb8"
   },
   "outputs": [],
   "source": [
    "# computes the WER for a list of models (columns) over a dataframe\n",
    "# returns a dataframe with WER scores for each model in the list\n",
    "def computeWERdf(df, models):\n",
    "  dfscores = pd.DataFrame(columns = ['model', 'WER'])\n",
    "  dfscores['model'] = models\n",
    "  # load metric\n",
    "  wer = load(\"wer\")\n",
    "  wer_scores = []\n",
    "  for model in models:\n",
    "    predictions = df[model]\n",
    "    references = df[\"reference\"]\n",
    "    wer_scores.append(wer.compute(predictions=predictions, references=references))\n",
    "\n",
    "  dfscores['WER'] = wer_scores\n",
    "  return dfscores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtWCTI1TEtA0"
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "computeWERdf(dftranscriptions,['stt_wav2vec2', 'sst_wav2vec2-base', 'stt_hubert', 'stt_whisper-base', 'stt_whisper-tiny'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvp5mugGGktQ"
   },
   "source": [
    "* These values indicate the average number of errors per reference word\n",
    "* Why whisper models are getting high WER scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_A7rhQsRzPe"
   },
   "source": [
    "### **Student activity #3:** Transcriptions issues\n",
    "1. Use the function `str.upper()` to fix the transcriptions generated by the whisper models and replace them in the dataframe.\n",
    "2. Compute the WER again for all the models and observe the changes in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAC0Nj-TGzc2"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "## Student activity solution #3\n",
    "###############################\n",
    "\n",
    "# Change to uppercase columns for whisper models\n",
    "dftranscriptions['stt_whisper-base'] = dftranscriptions['stt_whisper-base'].str.upper()\n",
    "dftranscriptions['stt_whisper-tiny'] = dftranscriptions['stt_whisper-tiny'].str.upper()\n",
    "\n",
    "# compute WER again\n",
    "computeWERdf(dftranscriptions,['stt_wav2vec2', 'sst_wav2vec2-base', 'stt_hubert', 'stt_whisper-base', 'stt_whisper-tiny'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nstVNRAXIsOy"
   },
   "source": [
    "**Whisper** models are capable of recognizing patterns in speech signals, including changes in intonation, pauses, and other cues that indicate the presence of punctuation. These models can generate text output that includes commas, periods, question marks, exclamation points, and other common punctuation marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkMGMviE0i6B"
   },
   "source": [
    "### Datasets in HuggingFace\n",
    "HuggingFace allows users to download and prepare datasets with a suite of functions that enable efficient pre-processing. As with ASR models, The Hub hosts a number of datasets for audio and speech tasks.\n",
    "\n",
    "Datasets can vary in terms of domain, language, tasks, and sizes ([ASR datasets](https://huggingface.co/datasets?task_categories=task_categories:automatic-speech-recognition&sort=downloads)).\n",
    "\n",
    "Datasets can be downloaded using the **load_dataset** function from **datasets** library. This library contains usefull methods to process dataset objects (e.g., **remove_column** method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n26JxsiPqxGE"
   },
   "source": [
    "### **Live coding:** Exploring datasets from HuggingFace\n",
    "\n",
    "1. Load the **ciempiess** dataset from The Hub repository and explore the elements included in it\n",
    "2. Pre-process the dataset using some of the methods available in the **dataset** library (e.g., remove columns, resample audio elements, filter elements in the dataset.)\n",
    "3. Present the **Streaming Mode** and understand its functionality\n",
    "\n",
    "\n",
    "For practicality purposes we selected the **ciempiess** dataset which is light enought to be downloaded during the live tutorial.\n",
    "\n",
    "\n",
    "The **CIEMPIESS TEST Corpus** is a gender balanced corpus designed to test acoustic models for the speech recognition task. It was created by recordings and human transcripts of 10 male and 10 female speakers. The language of the corpus is Spanish with the accent of Central Mexico except for the speaker M_09 that comes from El Salvador. More information available at [Ciempiess-HuggingFace](https://huggingface.co/datasets/ciempiess/ciempiess_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UMedaXBHlRE"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, Audio\n",
    "\n",
    "# Load and inspect datasets format in HuggingFace\n",
    "spanish_dataset = load_dataset(\"ciempiess/ciempiess_test\")\n",
    "\n",
    "print('Dataset features available:')\n",
    "print('===========================')\n",
    "print(spanish_dataset[\"test\"])\n",
    "print('===========================')\n",
    "\n",
    "\n",
    "print('Sample element from dataset:')\n",
    "print('============================')\n",
    "spanish_dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsLXh5h7Y7J5"
   },
   "source": [
    "* At a dataset object (*spanish_dataset[test]*) we have the **feature** headers included in our dataset (they can vary depending on the dataset) and the **num_rows** indicating the number of elements in the dataset.\n",
    "\n",
    "* For any ASR dataset, features can vary, but two elements are always included: **audio** (which has inner elements like path, array, and sampling_rate) and **transcription** (in this dataset we can see it labelled as **normalized_text**).\n",
    "\n",
    "\n",
    "The **remove_columns()** function allow removing columns from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dXzZHy8KBqU"
   },
   "outputs": [],
   "source": [
    "# Pre-processing: remove_columns function\n",
    "\n",
    "columns_to_remove = ['audio_id', 'speaker_id', 'gender', 'duration']\n",
    "spanish_dataset = spanish_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "print(spanish_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAviUqe1dVh3"
   },
   "source": [
    "The **cast_column()** function is used to cast a column to another feature to be decoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGyMBy3aKt3i"
   },
   "outputs": [],
   "source": [
    "# Pre-processing: Resampling\n",
    "\n",
    "from datasets import Audio\n",
    "\n",
    "spanish_dataset = spanish_dataset.cast_column(\"audio\", Audio(sampling_rate=8000))\n",
    "\n",
    "spanish_dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht5OMc5Db_8D"
   },
   "source": [
    "* The sampling_rate value has changed to 8000.\n",
    "* We can resample the audio column again and go back to a sampling_rate of 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJmPUjOBLY-L"
   },
   "outputs": [],
   "source": [
    "spanish_dataset = spanish_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "spanish_dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ide2pvtGcW25"
   },
   "source": [
    "We can filter elements from our dataset depending on the task we want to complete (e.g., duration of speech samples, gender of speaker, etc.). The **filter()** function returns rows that match a specified condition.\n",
    "\n",
    "We will filter out from our dataset audio samples that exceed a 10 seconds duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8v6sECELiRK"
   },
   "outputs": [],
   "source": [
    "# Pre-processing: Filtering\n",
    "\n",
    "spanish_dataset = load_dataset(\"ciempiess/ciempiess_test\")\n",
    "\n",
    "def is_audio_length_in_range(duration):\n",
    "    return duration < 10\n",
    "\n",
    "print('Before filtering:')\n",
    "print(spanish_dataset[\"test\"].num_rows)\n",
    "\n",
    "spanish_dataset[\"test\"] = spanish_dataset[\"test\"].filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
    "\n",
    "print('After filtering:')\n",
    "print(spanish_dataset[\"test\"].num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kxr1XT4MO9S6"
   },
   "source": [
    "What is the **Streaming Mode** in HuggingFace?\n",
    "\n",
    "* Big challenges in HuggingFace is the datasets' sizes.\n",
    "* GigaSpeech smallest configuration has 10 hours of training data at 13GB of storage. Other configurations with 10 K hours require over 1 TB of storage space.\n",
    "\n",
    "The **datasets** library allows the use of a **streaming** mode to load data progressively. In this mode, data is loaded sample by sample by iterating over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZT8YOK_dO8w0"
   },
   "outputs": [],
   "source": [
    "librispeech_asr = load_dataset(\"librispeech_asr\", split=\"test.clean\", streaming=True)\n",
    "print(next(iter(librispeech_asr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dF6hGD0iT6-"
   },
   "source": [
    "There is one caveat to streaming mode. Different from a traditional operation where we only have to perform the downloading and processing operations once, at streaming mode the data is not downloaded to disk, so any download and processing operations need to be repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvAW1ncJx1nz"
   },
   "source": [
    "# **Additional Material**\n",
    "\n",
    "\n",
    "* Fine-Tune Wav2Vec2 for English ASR with HuggingFace Transformers [post link](https://huggingface.co/blog/fine-tune-wav2vec2-english)\n",
    "* Automatic Speech Recognition example [post link](https://huggingface.co/docs/transformers/main/tasks/asr)\n",
    "* A complete guide to Audio Datasets [post link](https://huggingface.co/blog/audio-datasets)\n",
    "* Loading a dataset in HuggingFace [post link](https://huggingface.co/docs/datasets/v1.11.0/loading_datasets.html)\n",
    "* Stream Mode [post link](https://huggingface.co/docs/datasets/stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oofhLvpjK1k"
   },
   "source": [
    "---\n",
    "# Assignment Questions PL6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9q9Z1oUKtdg"
   },
   "source": [
    "Upload files from local system provided for this assignment (`PL6_files_assignment.zip`)\n",
    "\n",
    "The zip files contains the complete version of the TCD-VoIP dataset, plus a csv file `tcdvoip-transcriptions.csv` with the corresponding transcriptions for each wav file in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWE1KQHCK5vX"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "zipname = 'PL6_files_assignment.zip'\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzL4xI48K-qf"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zipname, 'r') as zip_ref:\n",
    "  zip_ref.extractall()  # Extract all files to the current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQMdWqU40xUn"
   },
   "source": [
    "## Assignment question 1\n",
    "Using the Hub platform at HuggingFace, look up for these ASR models and test them over the TCD-VoIP dataset (5pts)\n",
    "\n",
    "| Developer  | Model name               |\n",
    "|------------|--------------------------|\n",
    "| microsoft  | speecht5_asr             |\n",
    "| openai     | whisper-small            |\n",
    "| openai     | whisper-base.en          |\n",
    "| facebook   | data2vec-audio-base-100h |\n",
    "| facebook   | wav2vec2-base-960h       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctkOmkSOOjeu"
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "## Assignment question solution #1\n",
    "##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjBLL8nFOlEP"
   },
   "source": [
    "## Assignment question 2\n",
    "Report the WER scores comparing the results of all five (5) models. To do so, use a scatter plot to depict the models (x-axis) and the corresponding WER scores (y-axis). Add the corresponding labels to the plot. Make sure you're solving any issues that might be affecting the WER scores (e.g., case sensitivity). (3pts)\n",
    "\n",
    "Add a brief comment (100 words max.) reflecting on the models' performance. You can use the descriptive information available at HuggingFace to help compare the models. (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4_3sHYZOp28"
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "## Assignment question solution #2\n",
    "##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vb7FkZQOs4u"
   },
   "source": [
    "## Assignment question 3\n",
    "Report the WER scores comparing the results for all five (5) types of degradation (echo, clip, competing speaker, noise, chop). To do so, use a scatter plot to depict the type of degradation (x-axis) and the corresponding WER scores (y-axis). Add the corresponding labels to the plot. Make sure you're solving any issues that might be affecting the WER scores (e.g., case sensitivity). (3pts)\n",
    "\n",
    "Add a brief comment (100 words max.) reflecting on the results across different types of degradation. (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Git_8fOq0R34"
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "## Assignment question solution #3\n",
    "##################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNh70yOP4qY0Uf2NBSzUFnR",
   "provenance": [
    {
     "file_id": "1x_4DNBh7IP9zTv9CsnWKCYfAxDZFBfej",
     "timestamp": 1701315345402
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
